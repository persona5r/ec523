# -*- coding: utf-8 -*-
"""ec523-3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/151BXp2MKI5-DehzslamMQSP9WJlNWQNx
"""

!pip install -U transformers==4.41.2 torch scipy

from transformers import AutoTokenizer, AutoModelForSequenceClassification
from scipy.special import softmax
import torch

MODEL = "cardiffnlp/twitter-roberta-base-sentiment"

# âœ… å…³é”®å‚æ•°ï¼šé¿å…è®¿é—®ä¸å­˜åœ¨çš„ additional_chat_templates æ–‡ä»¶å¤¹
tokenizer = AutoTokenizer.from_pretrained(MODEL, trust_remote_code=True, revision="main")
model = AutoModelForSequenceClassification.from_pretrained(MODEL, trust_remote_code=True, revision="main")

def same_seeds(seed=87):
    import random, numpy as np, torch
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.benchmark = False
    torch.backends.cudnn.deterministic = True

from google.colab import drive
drive.mount('/content/drive')

path = '/content/drive/MyDrive/project/train.csv'
import pandas as pd
df = pd.read_csv(path)[['text','sentiment','selected_text']]
df = df.sample(frac=1, random_state=42).reset_index(drop=True)
df = df.dropna().reset_index(drop=True)
train_df = df.sample(frac=0.8, random_state=42)
val_df = df.drop(train_df.index)
train_text = train_df['text'].tolist()
train_sentiment = train_df['sentiment'].tolist()
train_selected_text = train_df['selected_text'].tolist()
val_text = val_df['text'].tolist()
val_sentiment = val_df['sentiment'].tolist()
val_selected_text = val_df['selected_text'].tolist()

import torch
from torch.utils.data import Dataset, DataLoader
from torch import nn, optim
import torch.nn as nn
import os
import pandas as pd

class LLMDataset(Dataset):
    def __init__(self, text, sentiment, selected_text = None, tokenizer = None, model = 'train', max_len=64):
      self.tokenizer = tokenizer
      self.model = model
      self.text = text
      self.sentiment = sentiment
      self.max_len = max_len # ä¿å­˜ max_len
      if self.model in ['train','val']:
        self.selected_text = selected_text

    def __getitem__(self, idx):
      # å› ä¸ºæˆ‘ä»¬ .dropna() äº†ï¼Œæ‰€ä»¥è¿™é‡Œ str() åªæ˜¯é¢å¤–çš„ä¿é™©
      text = str(self.text[idx])
      sentiment = str(self.sentiment[idx])

      # å…³é”®ä¿®å¤ï¼šåªè°ƒç”¨ä¸€æ¬¡ tokenizerï¼ŒåŒæ—¶è¯·æ±‚ offset
      # truncation=True ä¼šè‡ªåŠ¨æˆªæ–­æ‰€æœ‰å†…å®¹ï¼ŒåŒ…æ‹¬ offset_mapping
      xencoded = self.tokenizer(
                                  sentiment,
                                  text,
                                  truncation=True,
                                  padding='max_length',
                                  max_length=self.max_len,
                                  return_tensors='pt',
                                  return_offsets_mapping=True
                               )

      # åˆ†ç¦»æ¨¡å‹è¾“å…¥å’Œ offset åˆ—è¡¨
      encoded = {key: val.squeeze(0) for key, val in xencoded.items() if key != 'offset_mapping'}
      offsets = xencoded['offset_mapping'].squeeze(0).tolist()

      if self.model in ['train','val']:
        selected_text_str = str(self.selected_text[idx])

        start_char = text.find(selected_text_str)
        end_char = start_char + len(selected_text_str)

        # è®¾ç½®å®‰å…¨çš„é»˜è®¤å€¼ (4, 4)ï¼Œå³ text çš„ç¬¬ä¸€ä¸ª token
        # è¿™ä¼šå¤„ç†æ‰€æœ‰ "æ‰¾ä¸åˆ°" (start_char == -1) çš„æƒ…å†µï¼ˆæ¯”å¦‚æ‹¼å†™é”™è¯¯çš„è„æ•°æ®ï¼‰
        start = 4
        end = 4

        if start_char != -1: # åªæœ‰åœ¨ æ‰¾å¾—åˆ° çš„æƒ…å†µä¸‹æ‰å»æœç´¢
            start_found = False
            for i, (start_offset, end_offset) in enumerate(offsets):
              if i <= 3: # è·³è¿‡ [CLS], sentiment, [SEP], [SEP]
                continue
              if start_offset == 0 and end_offset == 0: # è·³è¿‡ padding
                continue

              if not start_found and start_offset <= start_char < end_offset:
                start = i
                start_found = True

              if start_found and start_offset < end_char <= end_offset:
                end = i
                break

            if start_found and end < start: # å¦‚æœ end æ²¡æ‰¾åˆ° (è¢«æˆªæ–­äº†)
                end = start

        # è¿”å›æ­£ç¡®çš„ tensor æ ¼å¼
        return encoded, torch.tensor([start, end])
      else:
        return encoded

    def __len__(self):
      return len(self.text)

class TweetExtractionModel(nn.Module):
    def __init__(self, model_name):
        super().__init__()
        from transformers import AutoModel
        self.roberta = AutoModel.from_pretrained(model_name)
        hidden = self.roberta.config.hidden_size
        self.dropout = nn.Dropout(0.1)
        self.start_fc = nn.Linear(hidden, 1)
        self.end_fc = nn.Linear(hidden, 1)

    def forward(self, input_ids, attention_mask):
        outputs = self.roberta(input_ids=input_ids, attention_mask=attention_mask)
        last_hidden = outputs.last_hidden_state    # [batch, seq_len, hidden]
        last_hidden = self.dropout(last_hidden)
        start_logits = self.start_fc(last_hidden).squeeze(-1)
        end_logits   = self.end_fc(last_hidden).squeeze(-1)
        return start_logits, end_logits

from tqdm import tqdm
import math
import numpy as np
model_name = "cardiffnlp/twitter-roberta-base-sentiment"
config = { 'seed':87,
          'batch_size':16,
           'n_epochs':4,
           'learning_rate':2e-5,
           'model_path':'./models/model.ckpt',
           'early_stop':1,
           'num_workers':4,
           'weight_decay':1e-5 }
device = 'cuda' if torch.cuda.is_available() else 'cpu'
max_len = 64 # ç¡®ä¿è¿™ä¸ªå€¼å’Œä½ çš„ tokenizer/config ä¸€è‡´
train_dataset = LLMDataset(train_text, train_sentiment, train_selected_text, tokenizer, 'train', max_len=max_len)
val_dataset = LLMDataset(val_text, val_sentiment, val_selected_text, tokenizer, 'val', max_len=max_len)
train_loader = DataLoader(train_dataset, batch_size=config['batch_size'], shuffle=True, num_workers=config['num_workers'])
val_loader = DataLoader(val_dataset, batch_size=config['batch_size'], shuffle=False, num_workers=config['num_workers'])
model = TweetExtractionModel(model_name)
model.to(device)
same_seeds(config['seed'])

from tqdm import tqdm
import math
import numpy as np
import torch
import os

def jaccard(str1, str2):
    """
    Calculates the Jaccard similarity score between two strings.
    """
    a = set(str(str1).lower().split())
    b = set(str(str2).lower().split())
    if not a and not b:
        return 1.0  # Both empty strings are a perfect match
    c = a.intersection(b)
    return float(len(c)) / (len(a) + len(b) - len(c)) if (len(a) + len(b) - len(c)) != 0 else 0.0


# --- è¯„ä¼°å‡½æ•° (å·²ä¿®å¤ç´¢å¼• Bug) ---
def evaluate_jaccard(model, val_loader, tokenizer, val_dataset, device, batch_size):
    """
    Evaluates the model's performance on the validation set using the Jaccard score.
    Returns both overall and sentiment-specific Jaccard averages.
    """
    model.eval()

    # --- å­˜å‚¨å„æƒ…æ„Ÿç±»åˆ«çš„jaccardæ€»å’Œä¸è®¡æ•° ---
    jaccard_sum = {"positive": 0.0, "negative": 0.0, "neutral": 0.0}
    jaccard_count = {"positive": 0, "negative": 0, "neutral": 0}

    total_jaccard = 0.0
    total_count = 0

    with torch.no_grad():
        for batch_idx, (encoded, y) in tqdm(enumerate(val_loader), total=len(val_loader), desc="Eval Jaccard"):
            encoded = {k: v.to(device) for k, v in encoded.items()}
            start_logits, end_logits = model(**encoded)

            start_pred = torch.argmax(start_logits, dim=1)
            end_pred = torch.argmax(end_logits, dim=1)

            for i in range(len(start_pred)):
                global_idx = batch_idx * batch_size + i
                if global_idx >= len(val_dataset.selected_text):
                    continue

                s = start_pred[i].item()
                e = end_pred[i].item()
                if e < s:
                    e = s

                input_ids_tensor = encoded["input_ids"][i]
                pred_tokens = input_ids_tensor[s : e + 1]
                pred_text = tokenizer.decode(pred_tokens)
                true_text = val_dataset.selected_text[global_idx]

                # è®¡ç®—å½“å‰æ ·æœ¬çš„ jaccard
                score = jaccard(pred_text, true_text)
                total_jaccard += score
                total_count += 1

                # --- æŒ‰æƒ…æ„Ÿç±»åˆ«ç´¯ç§¯ ---
                sentiment = val_dataset.sentiment[global_idx].lower()
                if sentiment in jaccard_sum:
                    jaccard_sum[sentiment] += score
                    jaccard_count[sentiment] += 1

    # --- å¹³å‡åŒ– ---
    overall = total_jaccard / total_count if total_count > 0 else 0.0
    sentiment_jaccard = {
        s: (jaccard_sum[s] / jaccard_count[s] if jaccard_count[s] > 0 else 0.0)
        for s in jaccard_sum
    }

    sentiment_jaccard["overall"] = overall
    return sentiment_jaccard


# --- è®­ç»ƒå‡½æ•° ---
def train(train_loader, val_loader, model, device, config, val_dataset, tokenizer):
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.AdamW(model.parameters(), lr=config['learning_rate'],weight_decay=config['weight_decay'])

    best_jaccard = 0.0
    save_path = config['model_path']
    early_stop = 0
    train_loss_list = []
    val_loss_list = []

    if not os.path.isdir('./models'):
        os.mkdir('./models')

    for epoch in range(config['n_epochs']):
        model.train()  # âœ… æ¯è½®å¼€å§‹å‰åˆ‡å›è®­ç»ƒæ¨¡å¼
        count = 0.0
        train_loss = 0.0

        # --- Training ---
        for i, data in tqdm(enumerate(train_loader), total=len(train_loader), desc=f"Epoch {epoch} Train"):
            optimizer.zero_grad()
            encoded, y = data
            encoded = {k: v.to(device) for k, v in encoded.items()}
            y = y.to(device)

            start, end = model(**encoded)
            start_loss = criterion(start, y[:, 0])
            end_loss = criterion(end, y[:, 1])
            loss = start_loss + end_loss
            loss.backward()
            optimizer.step()

            train_loss += loss.item()
            count += 1

        train_loss_list.append(train_loss / count)
        print(f"Epoch:{epoch} | train loss: {train_loss / count:.4f}")

        # --- Validation ---
        model.eval()
        val_loss = 0.0
        count = 0.0
        with torch.no_grad():
            for i, data in tqdm(enumerate(val_loader), total=len(val_loader), desc=f"Epoch {epoch} Val"):
                encoded, y = data
                encoded = {k: v.to(device) for k, v in encoded.items()}
                y = y.to(device)

                start, end = model(**encoded)
                start_loss = criterion(start, y[:, 0])
                end_loss = criterion(end, y[:, 1])
                loss = start_loss + end_loss

                val_loss += loss.item()
                count += 1

        val_loss_list.append(val_loss / count)
        val_jaccard_dict = evaluate_jaccard(model, val_loader, tokenizer, val_dataset, device, config['batch_size'])
        overall_jac = val_jaccard_dict["overall"]

        print(
              f"Epoch:{epoch} | val loss: {val_loss / count:.4f} | "
              f"Overall Jaccard: {overall_jac:.4f} | "
              f"Pos: {val_jaccard_dict['positive']:.4f} | "
              f"Neu: {val_jaccard_dict['neutral']:.4f} | "
              f"Neg: {val_jaccard_dict['negative']:.4f}"
              )

        # --- Early stopping logic ---
        if overall_jac > best_jaccard:
            best_jaccard = overall_jac
            early_stop = 0
            torch.save(model.state_dict(), save_path)
            print(f"âœ… Model saved at epoch {epoch}, Jaccard improved to {overall_jac:.4f}")
        else:
            early_stop += 1
            if early_stop >= config["early_stop"]:
                print(f"â¹ Early stop at epoch {epoch}")
                break

        model.train()  # âœ… éªŒè¯å®Œåå›åˆ°è®­ç»ƒæ¨¡å¼

    return train_loss_list, val_loss_list  # âœ… return æ”¾åœ¨å¾ªç¯å¤–

train_loss = []
val_loss = []
train_loss,val_loss = train(train_loader,val_loader,model,device,config,val_dataset,tokenizer)

import matplotlib.pyplot as plt

plt.figure(figsize=(8, 5))
plt.plot(train_loss, label='Train Loss', marker='o')
plt.plot(val_loss, label='Validation Loss', marker='s')
plt.title('Training and Validation Loss Curve')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.grid(True)
plt.show()

from tqdm import tqdm

model.eval()
N = 15  # åªçœ‹å‰5æ¡
shown = 0

with torch.no_grad():
    for i in tqdm(range(len(val_dataset)), desc="Sample predictions"):
        text = val_dataset.text[i]
        sentiment = val_dataset.sentiment[i]
        true_text = val_dataset.selected_text[i]

        # ğŸ”¹é‡æ–°è°ƒç”¨ tokenizer è·å– offset_mapping
        encoded = tokenizer(
            text,
            truncation=True,
            padding='max_length',
            max_length=max_len,
            return_offsets_mapping=True,
            return_tensors='pt'
        )

        # å–å‡ºæ¨¡å‹è¾“å…¥éƒ¨åˆ†
        encoded_to_model = {k: v.to(device) for k, v in encoded.items() if k in ('input_ids', 'attention_mask')}
        offsets = encoded["offset_mapping"][0].numpy()

        # ğŸ”¹æ¨¡å‹é¢„æµ‹
        start_logits, end_logits = model(**encoded_to_model)
        start_pred = torch.argmax(start_logits, dim=1).item()
        end_pred = torch.argmax(end_logits, dim=1).item()

        # ğŸ”¹ç¡®ä¿ s <= e
        s, e = min(start_pred, end_pred), max(start_pred, end_pred)
        start_char, end_char = int(offsets[s][0]), int(offsets[e][1])
        pred_text = text[start_char:end_char]

        # ğŸ”¹æ‰“å°ç»“æœ
        print(f"\nğŸŸ© Example {shown+1}")
        print(f"Sentiment: {sentiment}")
        print(f"Text: {text}")
        print(f"âœ… Predicted: {pred_text}")
        print(f"ğŸ¯ True: {true_text}")

        shown += 1
        if shown >= N:
            break

path = '/content/drive/MyDrive/project/test.csv'
import pandas as pd
df = pd.read_csv(path)[['text','sentiment']]
df = df.sample(frac=1, random_state=42).reset_index(drop=True)
df = df.dropna().reset_index(drop=True)
test_text = df['text'].tolist()
test_sentiment = df['sentiment'].tolist()

def test_model(model, tokenizer, test_texts, test_sentiments, device, max_len=64, num_samples=10):
    """
    ä½¿ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹åœ¨æµ‹è¯•/éªŒè¯é›†ä¸Šè¿›è¡Œæ¨ç†ï¼Œå¹¶æ‰“å°éƒ¨åˆ†æ ·æœ¬ç»“æœã€‚
    å‚æ•°:
        model: è®­ç»ƒå¥½çš„ TweetExtractionModel
        tokenizer: ä¸æ¨¡å‹å¯¹åº”çš„ tokenizer
        test_texts: æ–‡æœ¬åˆ—è¡¨
        test_sentiments: å¯¹åº”çš„æƒ…æ„Ÿåˆ—è¡¨
        device: 'cuda' æˆ– 'cpu'
        max_len: tokenizer æœ€å¤§é•¿åº¦
        num_samples: æ‰“å°çš„æ ·æœ¬æ•°é‡ï¼ˆé»˜è®¤10ï¼‰
    """

    model.eval()
    preds, spans = [], []
    n = len(test_texts)

    with torch.no_grad():
        for i in tqdm(range(n), desc="Testing"):
            text = test_texts[i]
            sentiment = test_sentiments[i]

            # é‡æ–°è°ƒç”¨ tokenizer ä»¥è·å¾— offset_mapping
            encoded = tokenizer(
                text,
                truncation=True,
                padding='max_length',
                max_length=max_len,
                return_offsets_mapping=True,
                return_tensors='pt'
            )

            offsets = encoded["offset_mapping"][0].numpy()
            encoded_to_model = {k: v.to(device) for k, v in encoded.items() if k in ("input_ids", "attention_mask")}

            # æ¨¡å‹é¢„æµ‹
            start_logits, end_logits = model(**encoded_to_model)
            start_pred = torch.argmax(start_logits, dim=1).item()
            end_pred = torch.argmax(end_logits, dim=1).item()

            # ä¿®æ­£ååº
            s, e = min(start_pred, end_pred), max(start_pred, end_pred)
            start_char, end_char = int(offsets[s][0]), int(offsets[e][1])
            pred_text = text[start_char:end_char]

            preds.append(pred_text)
            spans.append((s, e))

            # æ‰“å°éƒ¨åˆ†æ ·æœ¬
            if i < num_samples:
                print(f"\nğŸŸ© Example {i+1}")
                print(f"Sentiment: {sentiment}")
                print(f"Text: {text}")
                print(f"âœ… Predicted: {pred_text}")

    print(f"\nâœ… Finished! Generated {len(preds)} predictions.")
    return preds, spans

test_model(model,tokenizer,test_text,test_sentiment,device)

import pandas as pd
import torch
from tqdm import tqdm

def generate_submission(model, tokenizer, test_texts, test_sentiments, device, max_len=64, save_path='test.csv'):
    """
    åœ¨æµ‹è¯•é›†ä¸Šç”Ÿæˆé¢„æµ‹ç»“æœå¹¶ä¿å­˜ä¸º test.csv
    ç»“æœåŒ…å«: text, sentiment, selected_text
    """
    model.eval()
    preds = []

    with torch.no_grad():
        for text, sentiment in tqdm(zip(test_texts, test_sentiments), total=len(test_texts), desc="Generating test.csv"):
            # é‡æ–°tokenizeæ‹¿offset
            encoded = tokenizer(
                text,
                truncation=True,
                padding='max_length',
                max_length=max_len,
                return_offsets_mapping=True,
                return_tensors='pt'
            )
            offsets = encoded["offset_mapping"][0].numpy()
            encoded_to_model = {k: v.to(device) for k, v in encoded.items() if k in ("input_ids", "attention_mask")}

            # æ¨¡å‹é¢„æµ‹
            start_logits, end_logits = model(**encoded_to_model)
            start_pred = torch.argmax(start_logits, dim=1).item()
            end_pred = torch.argmax(end_logits, dim=1).item()

            s, e = min(start_pred, end_pred), max(start_pred, end_pred)
            start_char, end_char = int(offsets[s][0]), int(offsets[e][1])
            pred_text = text[start_char:end_char].strip()

            preds.append(pred_text)

    df = pd.DataFrame({
        "text": test_texts,
        "sentiment": test_sentiments,
        "selected_text": preds
    })

    df.to_csv(save_path, index=False, encoding='utf-8-sig')
    print(f"âœ… Saved {len(preds)} predictions to {save_path}")
    return df

model = TweetExtractionModel(model_name)
model.load_state_dict(torch.load('./models/model.ckpt', map_location=device))
model.to(device)

df_sub = generate_submission(model, tokenizer, test_text, test_sentiment, device, save_path='test.csv')